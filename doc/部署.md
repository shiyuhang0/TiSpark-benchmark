本文档介绍 TiDB & Spark 集群的部署与监控

# TiDB
使用 tiup cluster 进行部署 [参考这里](https://docs.pingcap.com/zh/tidb/stable/production-deployment-using-tiup)
1. 配置 topology.yaml


2. 环境与系统配置检查&修复 `tiup cluster check ./topology.yaml --user root [-p] [-i /home/root/.ssh/gcp_rsa]`


3. 手动修复无法自动修复的错误，如：
- 问题：numactl not usable, bash: numactl: command not found
- 解决：yum -y install numactl.x86_64
- 问题：mount point / does not have 'nodelalloc' option set
- 解决：[指定挂载参数挂盘](https://docs.pingcap.com/tidb/stable/check-before-deployment#mount-the-data-disk-ext4-filesystem-with-options-on-the-target-machines-that-deploy-tikv)


4. 部署 `tiup cluster deploy tidb-test v5.4.0 ./topology.yaml --user root [-p] [-i /home/root/.ssh/gcp_rsa]`


5. 启动 `tiup cluster start tidb-test`

# Spark
Spark 有多种部署方式，这里我以 standalone 模式进行介绍。

下文以1 master 3 worker 为例，四台机器 ip 分别为 172.16.201.185；172.16.201.200；172.16.201.174；172.16.201.14

前置条件
- 每台机器配置 JDK
- 配置 ssh 免密

以下操作在 master 做
1. 下载 spark 并解压
2. 配置 slaves
```
172.16.201.200
172.16.201.174
172.16.201.14
```
3. spark-env.conf 最小配置
```
SPARK_MASTER_HOST=172.16.201.185
JAVA_HOME=/root/jdk1.8.0_202
```
4. 配置 host 并关闭防火墙。(如果不能关闭防火墙需要开放 8080，8081 等端口）
```
172.16.201.200 vm172-16-201-200
172.16.201.174 vm172-16-201-174
172.16.201.14 vm172-16-201-14
172.16.201.185 vm172-16-201-185
```
4. 复制 spark 到其他三台 worker
5. 使用启动脚本启动
```
sbin/start-master.sh - Starts a master instance on the machine the script is executed on.
sbin/start-slaves.sh - Starts a worker instance on each machine specified in the conf/slaves file.
sbin/start-slave.sh - Starts a worker instance on the machine the script is executed on.
sbin/start-all.sh - Starts both a master and a number of workers as described above.
sbin/stop-master.sh - Stops the master that was started via the sbin/start-master.sh script.
sbin/stop-slave.sh - Stops all worker instances on the machine the script is executed on.
sbin/stop-slaves.sh - Stops all worker instances on the machines specified in the conf/slaves file.
sbin/stop-all.sh - Stops both the master and the workers as described above.
```

# Hadoop
如果需要 HDFS , 或 spark 使用 yarn 模式则安装 hadoop.

下文以 1 NameNode 3 DataNode 为例，四台机器 ip 分别为 172.16.201.185；172.16.201.200；172.16.201.174；172.16.201.14

1. [option] 创建 hadoop 用户，在 hadoop 用户下进行以下操作，和 root 用户隔离
2. 下载 hadoop
3. 配置 hadoop 环境变量
4. 配置 hadoop-env.sh
```
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export JAVA_HOME=/root/jdk1.8.0_202
```
5. 配置 core-site.xml
```
<configuration>
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://172.16.201.185:9000</value>
    <description>HDFS的URI，文件系统://namenode标识:端口号</description>
</property>

<property>
    <name>hadoop.tmp.dir</name>
    <value>/usr/local/hadoop/tmp</value>
    <description>namenode上本地的hadoop临时文件夹</description>
</property>
</configuration>
```
6. 配置 hdfs-site.xml
```
<configuration> 
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/data/hadoop_data/hdfs/namenode</value>    
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/data/hadoop_data/hdfs/datanode</value>        
    </property>
</configuration>
```
7. 配置 mapred-site.xml (如果需要 yarn)
```
<configuration> 
    <property> 
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property> 
</configuration>
```
8. 配置 yarn-site.xml (如果需要 yarn)
```
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
         <name>yarn.resourcemanager.hostname</name>
         <value>172.16.201.185</value>
    </property>
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>
```
9. 配置 workers
```
172.16.201.200
172.16.201.174
172.16.201.14
```
10. 复制 hadoop 到其他三个节点
11. 首次运行需格式化NameNode 文件系统： `hdfs namenode -format`
12. 启动 HDFS：`start-dfs.sh`
13. 启动 yarn (如果需要): `start-yarn.sh`


# 监控
TiDB

TiUP 提供监控部署，使用 grafana 查看 node metric 和 app metric

Spark （无现成 node 监控，最好添加 node-exporter 进行节点监控）
- 8080：master webUI
- 8081：worker webUI
- 4040：SparkContext webUI (用来查看作业情况)
- 18080：historyServer  webUI（查看已完成的作业）